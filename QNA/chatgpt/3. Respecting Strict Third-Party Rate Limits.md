Nice ‚Äî here‚Äôs the **Scenario 3** answer in the same easy-English, short-explainer + key-point style you asked for.

---

# üö¶ Scenario 3 ‚Äî Respecting Strict Third-Party Rate Limits (30 req/min)

You talk to a third-party API that **only allows 30 requests per minute**. Some requests fail temporarily or return rate-limit errors. How to build an internal queuing system that stays inside that limit, retries smartly, avoids overload, and shows users a friendly progress bar?

---

## 1) High-level design: single throttled dispatcher + durable queue

**Idea:** Accept all incoming work quickly, put jobs into a durable queue, and have a controlled dispatcher that **sends requests to the 3rd-party at ‚â§ 30 req/min**.

**Key points**

* **Ingress ‚Üí fast accept / enqueue** (HTTP 202 or 200). Don‚Äôt call third-party from request handler.
* **Durable queue** (SQS / Kafka / Rabbit / Redis Streams) holds jobs.
* **Throttled dispatcher** reads from queue but only issues requests at the allowed rate (30/min).
* **Worker pool** processes responses and marks job status (done, retry, failed).
* **State store** (DB or cache) tracks job state and ETA for the progress bar.

---

## 2) How to stay within 30 req/min (token-bucket style)

**Simple approach:** Build a **central token bucket** that refills 30 tokens every minute. A job needs one token to be sent.

**Key points**

* Centralized token store in Redis (atomic INCR/DECR or Lua script) so distributed dispatchers coordinate.
* If no token available ‚Üí job stays in queue (or is moved to a ‚Äúwaiting‚Äù queue).
* Use **leaky-bucket** or **fixed-window** if simpler; token-bucket gives smooth bursts (but still respects average).

**Tiny pseudocode**

```
every minute: set tokens = 30
while tokens > 0 and queue.not_empty():
   token = take_one_token_atomically()
   if token:
      job = queue.pop()
      send_to_third_party(job)
```

---

## 3) Prevent incoming overflow

**Key points**

* **Durable queue with large retention** prevents data loss (SQS, Kafka, Redis Streams).
* **Ingress rate-limiting**: apply soft limits at API gateway (429 when too many incoming requests) to avoid unbounded queue growth.
* **Per-tenant quotas:** stop one noisy tenant from filling the queue.
* **Backpressure signals:** if queue depth exceeds threshold, return 503/429 or show estimated wait time to caller.
* **Auto-scaling consumers** only affects internal processing, not third-party call rate ‚Äî prevent scaling consumers from bypassing the token bucket.

---

## 4) Retry failed jobs intelligently (without overwhelming API)

**Key points**

* **Respect Retry-After header** if the API returns it on 429.
* **Exponential backoff + jitter** for temporary errors (e.g., 1s, 2s, 4s, 8s + random jitter).
* **Rate-aware retries:** retries also need tokens ‚Äî they must re-enter the same token-bucket flow.
* **Max attempts ‚Üí DLQ:** after N tries, move to Dead-Letter Queue for manual review.
* **Adaptive cooldown:** if many 429s occur, reduce token rate temporarily (e.g., cut to 50%) and alert.
* **Use circuit breaker:** if API failing, stop sending new calls for a short window and drain queue more slowly.

**Retry flow example**

1. Send job ‚Äî got 429 + Retry-After=60 ‚Üí schedule retry after 60s, do not consume extra tokens until retry time.
2. Send job ‚Äî got 5xx ‚Üí schedule exponential-backoff retry and requeue.

---

## 5) Progress bar & ETA for users (friendly, realistic)

**Goal:** Show user where their job is and expected wait time.

**Key points**

* Track job **position** (or priority bucket) and **avg rate** (30/min) and **avg processing time**.
* **ETA calculation:** `estimated_wait = (jobs_ahead / effective_rate) + avg_processing_time`

  * `effective_rate` = allowed rate (30/min) * fraction allocated to this user/priority.
* Show **stages** (Queued ‚Üí Sending ‚Üí In-progress ‚Üí Done) and an ETA in minutes.
* **Real-time updates:** push updates via WebSocket or Server-Sent Events, or let client poll every few seconds.
* If job is retried, update ETA to reflect retry delay.
* Warn the user if ETA may change (e.g., ‚ÄúQueue size increased ‚Äî ETA now ~12 min‚Äù).

**Example UX fields**

* Status: `Queued` / `Waiting for API token` / `Retrying` / `Completed`
* ETA: `~ 8 min`
* Position: `#12 in queue`
* Button: `Cancel job`

---

## 6) Queue engines, libraries, and scheduling techniques (cloud-friendly)

**Managed cloud options**

* **AWS:** SQS (durable queue) + Lambda or ECS workers + DynamoDB for state. Use SQS visibility timeout + DLQ.
* **GCP:** Pub/Sub or Cloud Tasks + Cloud Functions / Cloud Run. Cloud Tasks has rate controls per queue.
* **Azure:** Service Bus + Functions.

**Open-source / self-hosted**

* **Redis Streams** or **Bull / BullMQ (Node.js)** ‚Äî good for rate-limiting via Redis.
* **RabbitMQ** ‚Äî good for priority queues and reliability.
* **Kafka** ‚Äî great for high-throughput, replayable streams; combine with a throttling service.
* **Celery (Python)**, **Sidekiq (Ruby)**, **Hangfire (.NET)** for worker frameworks.

**Scheduling / rate-control techniques**

* **Token bucket** in Redis (atomic Lua) ‚Äî distributed, simple to enforce exact rate.
* **Leaky-bucket** or **fixed-window** for simpler implementations.
* **Cloud-native rate-limited queues:** Cloud Tasks (GCP) or ability to schedule messages in SQS (DelaySeconds).
* **KEDA** for scaling consumers in Kubernetes based on queue length ‚Äî but ensure token-bucket remains central so scaling doesn‚Äôt break limits.
* **Scheduler service**: small scheduler app that drains queue at exact rate (cron-like or event-driven).

---

## 7) Extras ‚Äî monitoring, alerts, and safety

**Key points**

* Monitor queue depth, token consumption rate, retry counts, DLQ size, and 429/5xx rate.
* Alert when retries spike or DLQ grows.
* Use quotas per user to avoid noisy neighbors.
* Provide users with cancellation and clear ETA to reduce support load.

---

## 8) Short summary (one-liner)

Use a **durable queue + a central token-bucket dispatcher** that sends at ‚â§30 req/min, apply **ingress limits** to prevent overflow, retry with **exponential backoff + respect Retry-After**, and show users **position + ETA** via WebSocket updates. For tooling, use cloud-managed queues (SQS, Cloud Tasks) or Redis/RabbitMQ/Bull for a managed/self-hosted stack.

---

Want a short **token-bucket Lua script for Redis**, a **sample SQS + Lambda architecture diagram**, or a small **ETA calculation snippet** to paste in your interview notes?
